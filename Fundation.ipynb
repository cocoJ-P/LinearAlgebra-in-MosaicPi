{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477b130a-fdea-4e80-8804-0c7bdb2fc249",
   "metadata": {},
   "source": [
    "# 💡 MosaicPi_Guide: **\"Linear Algebra in MosaicPi\"**\n",
    "\n",
    "> All code and examples are shared to help researchers, students, and engineers understand the reasoning behind DDDA — and to make it easy to apply dimensional analysis to your own data.  \n",
    "> This notebook serves as an entry-level guide for teaching, validating physical models, and enabling domain-specific knowledge engineering through data-driven dimensional reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 What You'll Learn\n",
    "\n",
    "**MosaicPi中的线性代数应用**\n",
    "\n",
    "This notebook introduces the **fundamental concepts of linear algebra** that are widely used in applied mathematics, data science, physics, and engineering. The focus is on building an intuitive understanding alongside practical computations using NumPy.\n",
    "\n",
    "1. **物理模型，隐函数，流形**  \n",
    "   Understand why we reduce variables and how dimensional consistency enables model generalization.\n",
    "\n",
    "2. **变量组合**  \n",
    "   Encode physical units of input quantities using base units and build the D-matrix.\n",
    "\n",
    "3. **变量组合评估**  \n",
    "   Discover dimensionless groups by solving linear algebraic equations on the D-matrix.\n",
    "\n",
    "4. **显式化策略可视化**  \n",
    "   Learn to assess whether derived groups make physical and computational sense.\n",
    "\n",
    "5. **不确定性定量化**  \n",
    "   Set the stage for further steps in the DDDA pipeline including Pi-group selection, uncertainty quantification, and regime detection.\n",
    "\n",
    "---\n",
    "\n",
    "## 👤 Author\n",
    "\n",
    "- **Name**: Jiashun Pang  \n",
    "- **Created**: August 2025  \n",
    "- **Affiliation**: MosaicPi, open research notebook  \n",
    "- **Notebook Focus**:  \n",
    "  A hands-on exploration of dimensional analysis — from aggregated raw quantities to symbolic Pi-group discovery and preparation for downstream DDDA tasks.\n",
    "\n",
    "---\n",
    "\n",
    "📌 *This notebook is designed to be accessible for learners new to dimensional analysis, while also laying the foundation for advanced applications in the full MosaicPi pipeline.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacafcc2-bad4-4243-b767-b857257b1961",
   "metadata": {},
   "source": [
    "# 1. Linear Algebra Basics Notebook\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "1. **Vectors and Matrices**  \n",
    "   - Representation of vectors and matrices in NumPy.\n",
    "   - Basic operations and data structures.\n",
    "\n",
    "2. **Vector Norms**  \n",
    "   - $L^1$ norm (Manhattan distance).  \n",
    "   - $L^2$ norm (Euclidean distance).  \n",
    "   - $L^\\infty$ norm (Maximum absolute value).\n",
    "\n",
    "3. **Matrix Norms**  \n",
    "   - Frobenius norm (energy of the matrix).  \n",
    "   - Spectral norm (largest singular value).\n",
    "\n",
    "4. **Inner and Outer Products**  \n",
    "   - Inner product (dot product): geometric interpretation as projection and angle measure.  \n",
    "   - Outer product: building higher-rank structures from vectors.\n",
    "\n",
    "5. **Linear Mappings and Basis Change**  \n",
    "   - How matrices act as linear transformations.  \n",
    "   - Representing vectors in different bases.\n",
    "\n",
    "6. **Coordinate Transformations**  \n",
    "   - Rotation, scaling, and shear using transformation matrices.\n",
    "\n",
    "7. **Basis Orthogonalization (Gram–Schmidt)**  \n",
    "   - Process of turning a general basis into an orthogonal (or orthonormal) basis.  \n",
    "   - Key for numerical stability and eigen-decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "Each section contains **Python code examples** with NumPy, allowing you to experiment directly and visualize the underlying operations. This notebook serves as both a **reference guide** and a **hands-on tutorial** for refreshing or learning the essentials of linear algebra.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354407e2-9f71-445a-97f9-93916121bbfb",
   "metadata": {},
   "source": [
    "# Linear Algebra Basics Notebook\n",
    "\n",
    "## 1. Vectors and Matrices\n",
    "\n",
    "1. **向量 (Vector)**\n",
    "\n",
    "   * **对象**：表示一个量（点、方向、状态）。\n",
    "   * **直观理解**：\n",
    "\n",
    "     * 在物理里，速度向量表示“往哪里、快多少”。\n",
    "     * 在几何里，向量是空间中的“箭头”。\n",
    "   * **作用**：通常用来描述“运动/状态/位置”。\n",
    "\n",
    "2. **矩阵 (Matrix)**\n",
    "\n",
    "   * **对象**：表示一种线性变换（operator），告诉你如何把一个向量变成另一个向量。\n",
    "   * **直观理解**：\n",
    "\n",
    "     * 旋转矩阵：把所有向量都绕原点转一定角度。\n",
    "     * 缩放矩阵：把所有向量拉长或压缩。\n",
    "     * 剪切矩阵：把方形变成平行四边形。\n",
    "   * **作用**：通常用来描述“变化/变换规则”。\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 二者的关系\n",
    "\n",
    "* **向量是“被变换的对象”**\n",
    "\n",
    "  * 你把向量 $x$ 交给矩阵 $A$，得到一个新的向量 $Ax$。\n",
    "* **矩阵是“变换的工具”**\n",
    "\n",
    "  * 它定义了这个空间里所有向量如何一起被改变。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 一句话总结\n",
    "\n",
    "* **向量** → 静态的量，表示“点/方向/状态/运动”。\n",
    "* **矩阵** → 动态的规则，表示“如何把向量整体改变”。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04890b7-ed44-4af6-a79a-727cdcf8dc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector v: [1 2 3]\n",
      "Matrix M:\n",
      " [[1 2 3]\n",
      " [3 4 5]\n",
      " [5 6 7]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vector and matrix examples\n",
    "v = np.array([1, 2, 3])\n",
    "M = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n",
    "\n",
    "print(\"Vector v:\", v)\n",
    "print(\"Matrix M:\\n\", M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946edc6-dccb-438c-af08-1c9a3435f566",
   "metadata": {},
   "source": [
    "## 2. **Vector Norms**\n",
    "\n",
    "**范数（Norm）就是一种“长度/大小”的度量方式**，它把一个向量或矩阵映射为一个非负实数，用来回答“这个对象有多大”。\n",
    "\n",
    "具体作用可以总结为：\n",
    "\n",
    "1. **度量距离**\n",
    "\n",
    "   * 通过不同范数，可以定义不同的“距离”概念（直线距离、走方格距离、最大分量差距等）。\n",
    "   * 应用于几何、机器学习中的相似度计算。\n",
    "\n",
    "2. **衡量大小/误差**\n",
    "\n",
    "   * 向量范数衡量数据点的大小，矩阵范数衡量算子的作用强度。\n",
    "   * 常用于误差分析（\\$L^2\\$ → 平均误差，\\$L^\\infty\\$ → 最坏情况误差）。\n",
    "\n",
    "3. **正则化与约束**\n",
    "\n",
    "   * 优化问题中加入范数作为正则项，控制解的稀疏性、平滑性或稳定性。\n",
    "   * 例如：\\$L^1\\$ 正则（稀疏解）、\\$L^2\\$ 正则（平滑解）。\n",
    "\n",
    "4. **数值稳定性分析**\n",
    "\n",
    "   * 在数值计算中，通过范数定义矩阵的 **条件数**，用于判断算法的稳定性与敏感性。\n",
    "\n",
    "---\n",
    "\n",
    "👉 一句话总结：\n",
    "**范数的本质就是给向量或矩阵定义一个“大小”，并在此基础上建立距离、误差和稳定性分析的工具。**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **\\$L^1\\$ 范数 (Manhattan distance, 曼哈顿距离)**\n",
    "\n",
    "公式：\n",
    "\n",
    "$$\n",
    "\\|x\\|_1 = \\sum_{i=1}^n |x_i|\n",
    "$$\n",
    "\n",
    "* **直观意义**：从坐标原点到某点的“走方格”的距离，就像在纽约曼哈顿的街道上走，必须沿着横纵方向走。\n",
    "* **例子**：\n",
    "  向量 $x = (3, -4)$\n",
    "  $\\|x\\|_1 = |3| + |-4| = 7$\n",
    "  → 如果你要从 (0,0) 走到 (3,-4)，只能走横纵方向的话，总共要走 7 步。\n",
    "\n",
    "**应用场景**：稀疏模型（如 Lasso 回归）会用到 \\$L^1\\$ 范数，因为它倾向于产生稀疏解（很多系数被压缩为零）。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **\\$L^2\\$ 范数 (Euclidean distance, 欧几里得距离)**\n",
    "\n",
    "公式：\n",
    "\n",
    "$$\n",
    "\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}\n",
    "$$\n",
    "\n",
    "* **直观意义**：从原点到该点的“直线距离”，就是平时最熟悉的几何距离。\n",
    "* **例子**：\n",
    "  向量 $x = (3, -4)$\n",
    "  $\\|x\\|_2 = \\sqrt{3^2 + (-4)^2} = \\sqrt{25} = 5$\n",
    "  → 就是从 (0,0) 直接走一条直线到 (3,-4) 的最短距离。\n",
    "\n",
    "**应用场景**：常用于机器学习的损失函数，比如最小二乘法 (least squares)，因为它衡量的是误差的平方和。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **\\$L^\\infty\\$ 范数 (Maximum norm, 最大范数)**\n",
    "\n",
    "公式：\n",
    "\n",
    "$$\n",
    "\\|x\\|_\\infty = \\max_i |x_i|\n",
    "$$\n",
    "\n",
    "* **直观意义**：从原点到点的距离，用“最大分量”来衡量。换句话说，看你在所有坐标方向上哪一个最远。\n",
    "* **例子**：\n",
    "  向量 $x = (3, -4)$\n",
    "  $\\|x\\|_\\infty = \\max(|3|, |-4|) = 4$\n",
    "  → 意味着走到 (3,-4) 时，最大的坐标偏移量是 4。\n",
    "\n",
    "**应用场景**：常用于误差上界的分析。比如在数值计算中，\\$L^\\infty\\$ 范数衡量“最坏情况下的误差”。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔎 对比总结\n",
    "\n",
    "* \\$L^1\\$ → 走格子距离 → 强调“整体偏移量”。\n",
    "* \\$L^2\\$ → 直线距离 → 强调“几何上的真实距离”。\n",
    "* \\$L^\\infty\\$ → 最大坐标差 → 强调“最坏情况”。\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d269b78d-01c4-4e32-b9a7-67cc986fe251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm: 6.0\n",
      "L2 norm: 3.7416573867739413\n",
      "Infinity norm: 3.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "# L1 norm (Manhattan)\n",
    "l1_norm = norm(v, 1)\n",
    "# L2 norm (Euclidean)\n",
    "l2_norm = norm(v, 2)\n",
    "# Infinity norm (max-abs)\n",
    "inf_norm = norm(v, np.inf)\n",
    "\n",
    "\n",
    "print(\"L1 norm:\", l1_norm)\n",
    "print(\"L2 norm:\", l2_norm)\n",
    "print(\"Infinity norm:\", inf_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394a62fd-2288-420d-b61c-94b83b733356",
   "metadata": {},
   "source": [
    "## 3. **Matrix Norms**\n",
    "\n",
    "和向量范数一样，**矩阵范数**是衡量矩阵“大小”的工具，但它强调的是矩阵作为线性变换的作用强度或元素整体规模。\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Frobenius Norm**\n",
    "\n",
    "$$\n",
    "\\|A\\|_F = \\sqrt{\\sum_{i,j} |a_{ij}|^2}\n",
    "$$\n",
    "\n",
    "* **直观意义**：把矩阵所有元素展开成一个大向量，再算 \\$L^2\\$ 范数。\n",
    "* **几何解释**：衡量矩阵的“能量”或总信息量。\n",
    "* **应用**：常用于误差度量，比如比较近似矩阵与原矩阵的差距。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Spectral Norm**\n",
    "\n",
    "$$\n",
    "\\|A\\|_2 = \\sigma_{\\max}(A)\n",
    "$$\n",
    "\n",
    "* **直观意义**：矩阵作用在所有向量上的最大“放大倍数”。\n",
    "* **几何解释**：矩阵把单位球映射成椭球，谱范数就是椭球的最长半径。\n",
    "* **应用**：常用于稳定性和条件数分析，因为它反映了矩阵最强的线性拉伸能力。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Spectral Radius**\n",
    "\n",
    "$$\n",
    "\\rho(A) = \\max \\{|\\lambda| : \\lambda \\in \\sigma(A)\\}\n",
    "$$\n",
    "\n",
    "* **直观意义**：所有特征值的模中取最大值，表示矩阵在某个特征方向上的最大伸缩因子。\n",
    "* **几何解释**：刻画了矩阵在长期作用下的“最强增长模式”。\n",
    "* **应用**：\n",
    "\n",
    "  * **迭代法收敛性**：若迭代矩阵的谱半径 \\$\\rho(A)<1\\$，迭代过程收敛。\n",
    "  * **动力系统稳定性**：在离散系统 \\$x\\_{k+1}=Ax\\_k\\$ 中，若 \\$\\rho(A)<1\\$，解会趋向于零。\n",
    "  * **对比谱范数**：谱半径 ≤ 谱范数；当矩阵是正规矩阵（如对称矩阵）时，二者相等。\n",
    "\n",
    "---\n",
    "\n",
    "👉 **概括一句话**：\n",
    "\n",
    "* Frobenius norm → **整体能量**（看所有元素）。\n",
    "* Spectral norm → **最强作用**（看最大奇异值）。\n",
    "* Spectral radius → **长期主导效应**（看最大特征值模）。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb8f8bb3-1a14-4aa3-9aba-c7f1a2222ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius norm: 13.19090595827292\n",
      "Spectral norm: 13.15934799669309\n",
      "Spectral radius: 12.928203230275507\n"
     ]
    }
   ],
   "source": [
    "# Frobenius norm\n",
    "fro_norm = norm(M, 'fro')\n",
    "# Spectral norm (largest singular value)\n",
    "spectral_norm = norm(M, 2)\n",
    "\n",
    "print(\"Frobenius norm:\", fro_norm)\n",
    "print(\"Spectral norm:\", spectral_norm)\n",
    "\n",
    "# 计算谱半径\n",
    "eigenvalues = np.linalg.eigvals(M)\n",
    "spectral_radius = max(abs(eigenvalues))\n",
    "\n",
    "print(\"Spectral radius:\", spectral_radius)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa43ad2-9fa7-4196-b99d-ff9c8c9e15a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Inner Product and Outer Product\n",
    "\n",
    "### 1. **Inner Product（内积 / 点积）**\n",
    "\n",
    "公式（向量 $x, y \\in \\mathbb{R}^n$）：\n",
    "\n",
    "$$\n",
    "\\langle x, y \\rangle = x \\cdot y = \\sum_{i=1}^n x_i y_i\n",
    "$$\n",
    "\n",
    "* **几何解释**：\n",
    "\n",
    "  * 衡量两个向量的 **相似性**。\n",
    "  * $\\langle x, y \\rangle = \\|x\\|\\|y\\|\\cos\\theta$，可用来计算夹角。\n",
    "  * 内积大 → 向量方向接近；内积为 0 → 向量正交。\n",
    "* **应用**：投影、余弦相似度、能量计算。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Outer Product（外积）**\n",
    "\n",
    "公式（向量 $x \\in \\mathbb{R}^m, y \\in \\mathbb{R}^n$）：\n",
    "\n",
    "$$\n",
    "x \\otimes y = x y^T\n",
    "$$\n",
    "\n",
    "* **几何解释**：\n",
    "\n",
    "  * 生成一个 **矩阵/张量**，每个元素是分量的乘积。\n",
    "  * 用两个向量“拼接”出更高维的结构。\n",
    "* **应用**：\n",
    "\n",
    "  * 构造秩一矩阵（低秩近似的基本单元）。\n",
    "  * 在机器学习中用于表示协方差矩阵、注意力机制（query-key 外积）。\n",
    "\n",
    "---\n",
    "\n",
    "👉 **一句话总结**：\n",
    "\n",
    "* **内积**：衡量向量之间的关系（角度、投影、相似性）。\n",
    "* **外积**：用向量构造更高阶结构（矩阵、张量、低秩分解）。\n",
    "\n",
    "好问题 👌！你抓住了“向量的内积/外积”往往是初学重点，但其实在 **矩阵层面**也有对应的“内积/外积”概念。我们分开说：\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **矩阵的内积 (Matrix Inner Product)**\n",
    "\n",
    "对于两个相同大小的矩阵 $A, B \\in \\mathbb{R}^{m \\times n}$，定义 **Frobenius 内积**：\n",
    "\n",
    "$$\n",
    "\\langle A, B \\rangle_F = \\text{trace}(A^T B) = \\sum_{i=1}^m \\sum_{j=1}^n a_{ij} b_{ij}\n",
    "$$\n",
    "\n",
    "* **直观意义**：把矩阵看作“大向量”，逐元素相乘后求和。\n",
    "* **应用**：\n",
    "\n",
    "  * 用它定义 Frobenius 范数：$\\|A\\|_F = \\sqrt{\\langle A, A \\rangle_F}$。\n",
    "  * 在优化/机器学习里用作矩阵之间的“相似度度量”。\n",
    "\n",
    "👉 这就是向量内积的自然推广。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **矩阵的外积 (Matrix Outer Product)**\n",
    "\n",
    "严格来说，“外积”通常是 **向量和向量**之间的概念，得到的是矩阵。\n",
    "但如果你想推广到矩阵层面，可以有两种理解方式：\n",
    "\n",
    "1. **Kronecker 积 (Kronecker Product)**\n",
    "   对于矩阵 $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{p \\times q}$，\n",
    "\n",
    "   $$\n",
    "   A \\otimes B = \n",
    "   \\begin{bmatrix}\n",
    "   a_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\n",
    "   a_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n",
    "   \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "   a_{m1}B & a_{m2}B & \\cdots & a_{mn}B\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "   * 这是“矩阵版的外积”，常用于张量构造。\n",
    "   * 在量子力学、信号处理里很常见。\n",
    "\n",
    "2. **向量化后做外积**\n",
    "\n",
    "   * 先把矩阵展平成向量（vec 操作），然后再做普通外积：\n",
    "\n",
    "     $$\n",
    "     \\text{vec}(A) \\, \\text{vec}(B)^T\n",
    "     $$\n",
    "   * 得到一个更高维的矩阵，常用于机器学习中的特征交叉。\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 总结\n",
    "\n",
    "* **矩阵内积**：$\\langle A,B\\rangle = \\text{trace}(A^T B)$，类似向量点积。\n",
    "* **矩阵外积**：没有唯一标准，但常用 **Kronecker 积** 或 **向量化后做外积**。\n",
    "\n",
    "---\n",
    "\n",
    "👉 一句话：\n",
    "**矩阵的内积很自然（trace 公式），矩阵的外积则要么用 Kronecker 积，要么通过向量化来定义。**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d41d7-a6a0-4eab-b018-376719738628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inner product (dot product)\n",
    "inner = np.dot(v, v)\n",
    "\n",
    "# Outer product\n",
    "outer = np.outer(v, v)\n",
    "\n",
    "print(\"Inner product:\", inner)\n",
    "print(\"Outer product:\\n\", outer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acf8a6-8a8f-4c1e-a085-34c4c600062c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Linear Mappings and Basis Change\n",
    "\n",
    "### 1. **Linear Mappings（线性映射）**\n",
    "\n",
    "* **矩阵的角色**：矩阵不仅仅是数字表格，它本质上是 **线性变换的算子**。\n",
    "* **作用方式**：当矩阵 $A$ 作用在向量 $x$ 上时，得到的结果 $Ax$ 表示“把向量 $x$”在空间中 **拉伸、缩放、旋转或反射**。\n",
    "* **几何意义**：矩阵可以把单位正方形/单位圆变换成平行四边形/椭圆。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Basis Change（基变换）**\n",
    "\n",
    "* **基的意义**：基（basis）是描述空间中向量的一组“坐标尺”。\n",
    "* **基变换过程**：同一个向量，可以在不同基下有不同的坐标表示。\n",
    "\n",
    "  $$\n",
    "  [x]_{new} = P^{-1} [x]_{old}\n",
    "  $$\n",
    "\n",
    "  其中 $P$ 是基变换矩阵。\n",
    "* **应用**：\n",
    "\n",
    "  * 坐标系切换（如从直角坐标系到极坐标系）。\n",
    "  * 对角化、主成分分析（PCA），通过换基揭示数据的本质方向。\n",
    "\n",
    "---\n",
    "\n",
    "👉 **一句话总结**：\n",
    "\n",
    "* **矩阵**：是线性变换的工具，控制向量在空间中的形状变化。\n",
    "* **基变换**：是重新选择坐标系，用新的“尺子”来描述同一个向量或变换。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd72e2e-1eb2-4194-82c8-7d24c6ae1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear mapping: matrix acting on vector\n",
    "A = np.array([[2, 0], [0, 3]])\n",
    "x = np.array([1, 1])\n",
    "y = A @ x\n",
    "print(\"Linear mapping result y:\", y)\n",
    "\n",
    "# Basis change example\n",
    "P = np.array([[1, 1], [0, 1]])   # Change of basis matrix\n",
    "x_new_basis = np.linalg.inv(P) @ x\n",
    "print(\"Coordinates of x in new basis:\", x_new_basis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27bc4ce-78c9-4c56-9ea1-6b703a8d460a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Coordinate Transformation\n",
    "\n",
    "**坐标变换**是通过矩阵来对向量或图形进行操作，本质是对坐标系的重新刻画。\n",
    "\n",
    "### 常见的三类变换\n",
    "\n",
    "1. **Rotation（旋转）**\n",
    "\n",
    "   * 用正交矩阵（旋转矩阵）实现：\n",
    "\n",
    "     $$\n",
    "     R(\\theta) = \\begin{bmatrix} \n",
    "     \\cos\\theta & -\\sin\\theta \\\\ \n",
    "     \\sin\\theta & \\cos\\theta \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   * 保持向量长度和角度不变，只改变方向。\n",
    "\n",
    "2. **Scaling（缩放）**\n",
    "\n",
    "   * 用对角矩阵实现：\n",
    "\n",
    "     $$\n",
    "     S = \\begin{bmatrix} \n",
    "     s_x & 0 \\\\ \n",
    "     0 & s_y \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   * 在不同方向上拉伸或压缩。\n",
    "\n",
    "3. **Shear（剪切/错切）**\n",
    "\n",
    "   * 用非对角元实现：\n",
    "\n",
    "     $$\n",
    "     H = \\begin{bmatrix} \n",
    "     1 & k \\\\ \n",
    "     0 & 1 \n",
    "     \\end{bmatrix}\n",
    "     $$\n",
    "   * 把正方形变成平行四边形，角度发生改变但面积可能保持。\n",
    "\n",
    "---\n",
    "\n",
    "👉 **一句话总结**：\n",
    "坐标变换就是用矩阵来控制图形的 **方向（旋转）、大小（缩放）、形状（剪切）**。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12310145-d468-4a40-a724-516c8d991ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rotate a vector by 90 degrees counter-clockwise\n",
    "rotation_matrix = np.array([[0, -1], [1, 0]])\n",
    "vector = np.array([1, 0])\n",
    "rotated_vector = rotation_matrix @ vector\n",
    "\n",
    "print(\"Original vector:\", vector)\n",
    "print(\"Rotated vector:\", rotated_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec3ce1-b454-40d0-950f-73f8e94614ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Basis Orthogonalization (Gram–Schmidt)\n",
    "\n",
    "### 1. **问题背景**\n",
    "\n",
    "* 在许多场景中（如数值计算、信号处理、PCA），我们希望一组基底向量 **互相正交**，这样计算更简洁稳定。\n",
    "* 一般给定的一组向量并不正交，需要用 **Gram–Schmidt 正交化** 来处理。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Gram–Schmidt 过程**\n",
    "\n",
    "输入：一组线性无关向量 $\\{v_1, v_2, ..., v_n\\}$。\n",
    "输出：一组正交（或正交归一）基 $\\{u_1, u_2, ..., u_n\\}$。\n",
    "\n",
    "步骤：\n",
    "\n",
    "1. 取第一个向量：\n",
    "\n",
    "   $$\n",
    "   u_1 = v_1\n",
    "   $$\n",
    "2. 对后续每个向量，逐步减去它在已有正交基上的投影：\n",
    "\n",
    "   $$\n",
    "   u_k = v_k - \\sum_{j=1}^{k-1} \\frac{\\langle v_k, u_j \\rangle}{\\langle u_j, u_j \\rangle} u_j\n",
    "   $$\n",
    "3. 若需要正交**归一**基，再把每个 $u_k$ 归一化：\n",
    "\n",
    "   $$\n",
    "   e_k = \\frac{u_k}{\\|u_k\\|}\n",
    "   $$\n",
    "\n",
    "### 3. **线性无关**\n",
    "一组 **正交向量**\n",
    "\n",
    "$$\n",
    "u_1 = (1,0), \\quad u_2 = (0,1)\n",
    "$$\n",
    "\n",
    "* 内积：\n",
    "\n",
    "  $$\n",
    "  \\langle u_1, u_2 \\rangle = 1 \\cdot 0 + 0 \\cdot 1 = 0\n",
    "  $$\n",
    "* ✅ 它们正交（互相垂直），而且显然线性无关。\n",
    "\n",
    "---\n",
    "\n",
    "一组 **线性无关但不正交的向量**\n",
    "\n",
    "$$\n",
    "v_1 = (1,0), \\quad v_2 = (1,1)\n",
    "$$\n",
    "\n",
    "* 内积：\n",
    "\n",
    "  $$\n",
    "  \\langle v_1, v_2 \\rangle = 1 \\cdot 1 + 0 \\cdot 1 = 1 \\neq 0\n",
    "  $$\n",
    "* ❌ 它们不正交（不垂直）。\n",
    "* 但它们 **线性无关**，因为：\n",
    "\n",
    "  $$\n",
    "  v_2 \\neq \\alpha v_1 \\quad \\text{对任意实数 }\\alpha\n",
    "  $$\n",
    "\n",
    "  （即 $v_2$ 不是 $v_1$ 的倍数）。\n",
    "\n",
    "👉 这说明：这两个向量虽然“歪歪斜斜”，但依然能张成整个二维平面。\n",
    "\n",
    "---\n",
    "直观解释\n",
    "\n",
    "* “线性无关” = 它们指向不同的方向，能覆盖更大的空间。\n",
    "* “正交” = 不仅方向不同，而且完全没有重叠分量。\n",
    "* 所以：\n",
    "\n",
    "  * 所有正交向量组一定线性无关。\n",
    "  * 但并不是所有线性无关向量组都正交。\n",
    "\n",
    "---\n",
    "\n",
    "✅ 一句话总结：\n",
    "$(1,0)$ 和 $(1,1)$ 是 **线性无关但不正交** 的典型例子。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **直观解释**\n",
    "\n",
    "* 就像把一组“歪斜”的向量，逐个“投影并减掉重叠部分”，最后得到一组互相垂直的“新尺子”。\n",
    "* **几何意义**：在原空间不变的前提下，重新选一组彼此正交的基。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **应用**\n",
    "\n",
    "* QR 分解的基础（矩阵分解、数值稳定计算）。\n",
    "* 简化几何或代数运算（正交基下点积、投影更方便）。\n",
    "* 数据降维（PCA 中的正交化步骤）。\n",
    "\n",
    "---\n",
    "\n",
    "👉 **一句话总结**：\n",
    "Gram–Schmidt 正交化就是一种系统方法，把任意一组线性无关向量“修正”为互相正交（或标准正交）的基底。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b102a018-4eb2-4b61-b088-e50635e818ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original basis:\n",
      " [[1. 1.]\n",
      " [1. 0.]]\n",
      "Orthogonalized basis:\n",
      " [[ 1.   1. ]\n",
      " [ 0.5 -0.5]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gram_schmidt(vectors):\n",
    "    orthogonal = []\n",
    "    for v in vectors:\n",
    "        w = v - sum(np.dot(v, u)/np.dot(u, u) * u for u in orthogonal)\n",
    "        if np.linalg.norm(w) > 1e-10:\n",
    "            orthogonal.append(w)\n",
    "    return np.array(orthogonal)\n",
    "\n",
    "# Example basis\n",
    "B = np.array([[1, 1], [1, 0]], dtype=float)\n",
    "orthogonal_basis = gram_schmidt(B)\n",
    "\n",
    "print(\"Original basis:\\n\", B)\n",
    "print(\"Orthogonalized basis:\\n\", orthogonal_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d0ca6-d4ec-4780-9bba-5104dfe04e87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
